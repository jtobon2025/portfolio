<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Python Code Display</title>
    <!-- Incluyendo Highlight.js y su tema -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</head>
<body>

    <header>
        <nav>
            <h1>Analysis and Optimization of an Investment Portfolio Using Machine Learning Models</h1>
        </nav>
    </header>

    <main>
        <h2>Python Code</h2>

        <!-- Bloque de Código -->
        <pre><code class="language-python">
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer  # Importación corregida
from joblib import Parallel, delayed
from xgboost import XGBClassifier
from sklearn.decomposition import PCA
import yfinance as yf
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from pypfopt import EfficientFrontier, risk_models, expected_returns, objective_functions
import quantstats as qs
import os

# Configuración de la ruta de salida para reportes HTML
output_directory = r"C:\gpp_v1"
os.makedirs(output_directory, exist_ok=True)

# Solicitar parámetros del inversionista
investment_amount = float(input("Ingrese el monto que desea invertir en USD: "))
expected_return = float(input("Ingrese la rentabilidad esperada anual en %: ")) / 100
risk_aversion = input("Ingrese el nivel de aversión al riesgo (alta, moderada, baja): ").lower()

# Configurar los umbrales según la aversión al riesgo
risk_threshold = {
    "alta": 0.025,
    "moderada": 0.035,
    "baja": 0.045
}.get(risk_aversion, 0.035)

# Función para descargar datos de activos con reducción del rango de fechas
def fetch_data(tickers):
    try:
        # Limitar el rango de datos a los últimos 5 años para mejorar el rendimiento
        data = yf.download(tickers, start="2019-01-01", end="2024-09-06")["Adj Close"]
        data.dropna(axis=1, inplace=True)
        returns = data.pct_change().dropna()
        available_tickers = data.columns.tolist()
        print(f"Activos con datos disponibles: {available_tickers}")
        return data, returns, available_tickers
    except Exception as e:
        print(f"Error al descargar los datos: {e}")
        return pd.DataFrame(), pd.DataFrame(), []

# Lista de tickers
tickers = [
    "AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "NVDA",  # Tecnología
    "JNJ", "PFE", "MRK", "ABT", "BMY",  # Salud
    "XOM", "CVX", "BP", "TTE", "SLB",  # Energía
    "PG", "KO", "PEP", "MDLZ", "WMT",  # Consumo
    "BA", "GE", "MMM", "HON", "LMT",  # Industriales
    "JPM", "GS", "MS", "C", "BAC"   # Financieros
]

# Descargar datos y filtrar tickers con datos disponibles
data, returns, available_tickers = fetch_data(tickers)

# Funciones de indicadores técnicos
def calculate_rsi(series, window=14):
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_macd(series, fast=12, slow=26, signal=9):
    exp1 = series.ewm(span=fast, adjust=False).mean()
    exp2 = series.ewm(span=slow, adjust=False).mean()
    macd = exp1 - exp2
    signal_line = macd.ewm(span=signal, adjust=False).mean()
    return macd, signal_line

def calculate_stochastic(series, window=14):
    low_min = series.rolling(window=window).min()
    high_max = series.rolling(window=window).max()
    stochastic = 100 * (series - low_min) / (high_max - low_min)
    return stochastic

# Función para calcular características de un solo ticker
def calculate_ticker_features(ticker, data):
    features = pd.DataFrame(index=data.index)
    features[f'{ticker}_rsi'] = calculate_rsi(data[ticker])
    macd, signal = calculate_macd(data[ticker])
    features[f'{ticker}_macd'] = macd
    features[f'{ticker}_signal'] = signal
    features[f'{ticker}_stochastic'] = calculate_stochastic(data[ticker])
    return features

# Función para crear características con procesamiento paralelo y reducir carga de datos
def create_features_parallel(data):
    # Procesar solo una muestra de tickers si el rendimiento es un problema
    sample_tickers = data.columns[:10]  # Reducir a los primeros 10 tickers para prueba
    results = Parallel(n_jobs=-1)(delayed(calculate_ticker_features)(ticker, data) for ticker in sample_tickers)

    # Combinar los resultados
    features = pd.concat(results, axis=1)
    features.fillna(method='ffill', inplace=True)
    features.fillna(method='bfill', inplace=True)

    print(f"Características creadas - shape: {features.shape}")
    return features

# Crear características utilizando procesamiento paralelo
if not data.empty:
    features = create_features_parallel(data)

# Función para entrenar y ajustar modelos de ML con reducción de dimensionalidad
def train_ml_models(features, returns):
    if features.empty or returns.empty:
        raise ValueError("Las características o el objetivo están vacíos después del preprocesamiento.")

    features = features.dropna(axis=1, how='all')
    imputer = SimpleImputer(strategy='mean')
    features_array = imputer.fit_transform(features)
    features = pd.DataFrame(features_array, index=features.index, columns=features.columns)

    target = (returns > 0).astype(int)
    features, target = features.align(target, join='inner', axis=0)

    if features.empty or target.empty:
        raise ValueError("Las características o el objetivo están vacíos después de la alineación.")

    # Reducción de dimensionalidad con PCA
    pca = PCA(n_components=0.95)  # Mantener el 95% de la varianza
    features_pca = pca.fit_transform(features)
    print(f"Reducción de dimensionalidad con PCA: {features_pca.shape}")

    # Usar solo una submuestra para acelerar el ajuste inicial del modelo
    X_train, X_test, y_train, y_test = train_test_split(features_pca, target, test_size=0.3, random_state=42)

    scaler = MinMaxScaler()  # Usar MinMaxScaler para normalización avanzada
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Entrenamiento y ajuste de RandomForest con una submuestra
    rf_param_grid = {'n_estimators': [50], 'max_depth': [10]}  # Parámetros simplificados
    rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=3, scoring='accuracy')
    rf_grid_search.fit(X_train, y_train)
    rf_model = rf_grid_search.best_estimator_
    rf_score = rf_model.score(X_test, y_test)
    print(f"Mejores hiperparámetros RandomForest: {rf_grid_search.best_params_}")
    print("Precisión del modelo RandomForest:", rf_score)

    # Entrenamiento y ajuste de XGBoost con una submuestra
    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
    xgb_param_grid = {'n_estimators': [50], 'max_depth': [3]}  # Parámetros simplificados
    xgb_grid_search = GridSearchCV(xgb_model, xgb_param_grid, cv=3, scoring='accuracy')
    xgb_grid_search.fit(X_train, y_train)
    xgb_best_model = xgb_grid_search.best_estimator_
    xgb_score = xgb_best_model.score(X_test, y_test)
    print(f"Mejores hiperparámetros XGBoost: {xgb_grid_search.best_params_}")
    print("Precisión del modelo XGBoost:", xgb_score)

    return rf_model, rf_score, xgb_best_model, xgb_score

# Entrenar los modelos
rf_model, rf_score, xgb_model, xgb_score = None, None, None, None
if not features.empty and not returns.empty:
    try:
        rf_model, rf_score, xgb_model, xgb_score = train_ml_models(features, returns)
    except ValueError as e:
        print(f"Error: {e}")

# Continuar con la optimización del portafolio y generación de reportes...

# Continuar con la optimización del portafolio y generación de reportes...
def optimize_portfolio(mu, S, expected_return):
    ef = EfficientFrontier(mu, S)
    ef.add_objective(objective_functions.L2_reg, gamma=0.1)
    ef.min_volatility()
    weights = ef.clean_weights()
    return pd.Series(weights)

# Calcular matriz de covarianza y rendimientos esperados
mu = expected_returns.mean_historical_return(data)
S = risk_models.CovarianceShrinkage(data).ledoit_wolf()
weights_series = optimize_portfolio(mu, S, expected_return)
weights_series /= weights_series.sum()

print("\nPesos iniciales asignados por el optimizador:")
print(weights_series)

# Calcular la distribución del capital
allocation = weights_series * investment_amount
print("\nDistribución del Capital:")
for ticker in allocation.index:
    shares = allocation[ticker] / data[ticker].iloc[-1]
    print(f"{ticker}: ${allocation[ticker]:.2f} -> {shares:.2f} acciones")

# Visualización y Reporte con QuantStats (mantener la lógica existente)
# ...

# Calcular el rendimiento de la cartera optimizada ajustada
portfolio_returns = returns.dot(weights_series)

# Descargar los benchmarks QQQ y SPY y calcular los rendimientos diarios
benchmark_qqq = yf.download("QQQ", start="2016-01-01", end="2024-09-06")["Adj Close"].pct_change().dropna()
benchmark_spy = yf.download("SPY", start="2016-01-01", end="2024-09-06")["Adj Close"].pct_change().dropna()

# Sincronizar datos y ajustarlos para comparaciones adecuadas
portfolio_returns.index = portfolio_returns.index.tz_localize(None)
benchmark_qqq.index = benchmark_qqq.index.tz_localize(None)
benchmark_spy.index = benchmark_spy.index.tz_localize(None)

# Alinear fechas comunes entre la estrategia y los benchmarks
common_dates = portfolio_returns.index.intersection(benchmark_qqq.index).intersection(benchmark_spy.index)
portfolio_returns = portfolio_returns.loc[common_dates]
benchmark_qqq = benchmark_qqq.loc[common_dates]
benchmark_spy = benchmark_spy.loc[common_dates]

# Calcular los retornos acumulados
strategy_cum_returns = (1 + portfolio_returns).cumprod() - 1
qqq_cum_returns = (1 + benchmark_qqq).cumprod() - 1
spy_cum_returns = (1 + benchmark_spy).cumprod() - 1

# Crear gráfico interactivo con Plotly
fig = go.Figure()
fig.add_trace(go.Scatter(x=strategy_cum_returns.index, y=strategy_cum_returns, mode='lines', name='Strategy'))
fig.add_trace(go.Scatter(x=qqq_cum_returns.index, y=qqq_cum_returns, mode='lines', name='QQQ'))
fig.add_trace(go.Scatter(x=spy_cum_returns.index, y=spy_cum_returns, mode='lines', name='SPY'))

# Configuración del gráfico
fig.update_layout(
    title='Cumulative Returns vs Benchmarks',
    xaxis_title='Fecha',
    yaxis_title='Retornos Acumulados',
    template='plotly_white'
)

# Guardar el gráfico como archivo HTML
plotly_output_path = os.path.join(output_directory, "cumulative_returns_plot.html")
fig.write_html(plotly_output_path)
print(f"\nEl gráfico de retornos acumulados ha sido guardado como '{plotly_output_path}'")

# Guardar el reporte completo de QuantStats como HTML
qs_report_path = os.path.join(output_directory, "quantstats_report.html")
adjusted_portfolio_returns = portfolio_returns * (1 + expected_return)
qs.reports.html(adjusted_portfolio_returns, benchmark=benchmark_qqq, output=qs_report_path)

print(f"\nEl reporte de QuantStats ha sido generado y guardado como '{qs_report_path}'")
        </code></pre>
    </main>
    <a href="project_1.html" class="back-button">Return to Home</a>
    <footer>
        <p>© 2024 Jorge Tobón. All rights reserved.</p>
    </footer>

</body>
</html>
